{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be00f55",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-15T18:20:45.517505Z",
     "iopub.status.busy": "2024-10-15T18:20:45.516962Z",
     "iopub.status.idle": "2024-10-15T18:20:46.644596Z",
     "shell.execute_reply": "2024-10-15T18:20:46.643575Z"
    },
    "papermill": {
     "duration": 1.135541,
     "end_time": "2024-10-15T18:20:46.647262",
     "exception": false,
     "start_time": "2024-10-15T18:20:45.511721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import string\n",
    "import lxml\n",
    "import time\n",
    "import os\n",
    "import lxml.html as lhtml\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "from utils import save_df_as_pretty_html\n",
    "from plotly import express as px\n",
    "# Base URL pattern for dream moods dictionary\n",
    "\n",
    "\n",
    "# List to store dream symbols and meanings\n",
    "dreams = []\n",
    "meanings = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfa61ac",
   "metadata": {},
   "source": [
    "## Data Scraping from dreammoods.com\n",
    "\n",
    "The website structure is fairly simple. Dream symbols are grouped into pages of their first letter:  \n",
    "http://www.dreammoods.com/dreamdictionary/c_all.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae23259",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_page(url, filename ):\n",
    "    if os.path.exists(filename):\n",
    "        print (f\"File {filename} already exists, skipping download.\")\n",
    "        return 'skip'\n",
    "    \n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:  # Check if the page was successfully fetched\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(response.text)\n",
    "        print(f\"Downloaded {url} to {filename}\")\n",
    "        return 'ok'\n",
    "    \n",
    "    else:        \n",
    "        newurl = url.replace(\"_all\",\"\")\n",
    "        if newurl != url:\n",
    "            print(f\"Failed to retrieve {url}, trying alternative\")\n",
    "            download_page(newurl, filename)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url} and no alternative available.\")\n",
    "            return 'fail'\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335e352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_url = \"http://www.dreammoods.com/dreamdictionary/\"\n",
    "suffix = \"{}_all.htm\"\n",
    "\n",
    "for letter in string.ascii_lowercase:\n",
    "\n",
    "    file = f\"html/{letter}_all.html\"\n",
    "\n",
    "    url = base_url + suffix.format(letter)\n",
    "\n",
    "    download_page(url, file)\n",
    "    time.sleep(.5)  # Sleep for 1 second to avoid overwhelming the server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849e7eb7",
   "metadata": {},
   "source": [
    "### Custom scraper\n",
    "Their HTML structure, however, is anything but simple. So we had to build a highly customized scraper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae99e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this string to debug code near specific text in html\n",
    "breakpointstring = \"your anchor text\"\n",
    "\n",
    "def brpt_anchor(val):\n",
    "    #if re.search(breakpointstring, tc): \n",
    "    if breakpointstring.lower() in str(val).lower():\n",
    "        print(\"found anchor text: \", breakpointstring, \", value: \", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53577f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def break_attr(dic):\n",
    "    drop = ['style','align','face']\n",
    "    for dr in drop:\n",
    "        if dr in dic:\n",
    "            dic.pop(dr)\n",
    "    return str(dic)\n",
    "\n",
    "def classify(t):\n",
    "    \n",
    "    brpt_anchor(t.text)\n",
    "        \n",
    "    \n",
    "    if type(t) is lxml.html.HtmlComment:\n",
    "        return \"trsh\"\n",
    "\n",
    "    if t.tag == \"b\":\n",
    "        return \"mean\" #\"bold\"\n",
    "    \n",
    "    trashtags = [\"div\",\"script\",\"iframe\",\"img\"]\n",
    "\n",
    "    if t.tag == \"strong\":\n",
    "        return \"sym\" #\"bold\"    \n",
    "\n",
    "    elif t.tag == \"font\":\n",
    "\n",
    "        if 'size' in t.attrib:\n",
    "            sz = t.attrib['size']\n",
    "            if sz == \"4\" or sz == \"+1\":\n",
    "                return \"sym\" #\"size4\"\n",
    "            elif sz == \"3\":\n",
    "                return \"mean\" #\"size3\"\n",
    "        else:\n",
    "            return \"mean\" # no size defined\n",
    "            \n",
    "    elif t.tag == \"a\":\n",
    "        if 'href' in t.attrib:\n",
    "            if t.attrib['href'] == \"#Top\":\n",
    "                return \"trsh\"\n",
    "            return \"link\"\n",
    "        elif 'name' in t.attrib:\n",
    "            return \"sym\" #\"header\"\n",
    "    \n",
    "    elif t.tag in trashtags:\n",
    "        return \"trsh\"\n",
    "    \n",
    "    return \"mean\" # \"desc\"\n",
    "\n",
    "def fetch_link(chld):\n",
    "\n",
    "    if chld.tag == \"a\":\n",
    "        if 'href' in chld.attrib:\n",
    "            return ',' + chld.attrib['href'] + ','\n",
    "            \n",
    "    else:\n",
    "        for c in chld.getchildren():\n",
    "            href = fetch_link(c)\n",
    "            if href != \"\":\n",
    "                return href\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def collect(node, cl):\n",
    "\n",
    "    brpt_anchor(node.text)\n",
    "\n",
    "    if \"trsh\" in cl:\n",
    "        return \"\"    \n",
    "    \n",
    "    if \"link\" in cl:\n",
    "        res = fetch_link(node)\n",
    "        return res\n",
    "    \n",
    "    if node.text == None:\n",
    "        return \"\"\n",
    "\n",
    "    else:\n",
    "        #res = node.text_content()\n",
    "        res = str(node.text  or '')+  str(node.tail or'')\n",
    "        res = res.replace(\"\\xa0\",\" \")\n",
    "        res = re.sub(r\"\\n+\",\"\\n\",res)\n",
    "        res = re.sub(r'\\s+', ' ', res)\n",
    "        if cl == \"mean\":\n",
    "            res = res + \" \"\n",
    "        return res\n",
    "\n",
    "\n",
    "def breakdown(node):\n",
    "    \n",
    "    brpt_anchor(node.text_content())\n",
    "\n",
    "    clas = [classify(node)]\n",
    "    cont = [collect(node, clas[0])]\n",
    "\n",
    "    if \"trsh\" in clas[-1]:\n",
    "        return [\"trsh\"], \"\"\n",
    "    \n",
    "    for c in node.getchildren():\n",
    "        \n",
    "        cl, co = breakdown(c)\n",
    "\n",
    "        clas += cl\n",
    "        cont += co\n",
    "\n",
    "    return clas, cont\n",
    "\n",
    "\n",
    "def extract_reference(cont):\n",
    "    syms = []\n",
    "    for c in cont:\n",
    "        c= c.lower()\n",
    "        if \"dreammoods.com/dreamdictionary\" in c and \"#\" in c:\n",
    "            syms.append(\"#Ref:\" + c.split(\"#\")[-1].strip(\",\"))\n",
    "        elif \"lease\" in c or \"see\" in c:\n",
    "            if \"also\" in c:\n",
    "                return None\n",
    "            #syms.append(c)\n",
    "    return syms[0] if len(syms)>0 else None\n",
    "\n",
    "\n",
    "\n",
    "def collect_paragraphs(r):\n",
    "    syms_data = []\n",
    "    for c in r.getchildren():\n",
    "        clas, cont = breakdown(c)\n",
    "        fclas = \"sym\" if \"sym\" in clas else \"link\" if \"link\" in clas else \"mean\"\n",
    "        \n",
    "        if fclas == \"link\":\n",
    "            text = extract_reference(cont)\n",
    "        else:\n",
    "            text = \"\".join(cont).strip()\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "        syms_data.append({\"clas\": clas, \"final_class\":fclas, \"cont\": cont, \"text\": text})\n",
    "\n",
    "    return syms_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf6bbe",
   "metadata": {},
   "source": [
    "Test with a single file, e.g. c_all.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c9b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpointstring = \"door kno\"\n",
    "\n",
    "with open(\"html/dreamdictionary/d_all.html\", \"r\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "html = html.replace(\"\\xa0\",\" \")\n",
    "\n",
    "xp = \"/html/body/table[2]/tr/td/div/center/table/tr[4]/td[1]\"\n",
    "\n",
    "tree = lhtml.fromstring(html)\n",
    "\n",
    "tabl = tree.xpath(xp)[0]\n",
    "\n",
    "sd = collect_paragraphs(tabl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a54617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = pd.DataFrame(sd)\n",
    "sd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b20be5",
   "metadata": {},
   "source": [
    "Extract links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d9338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = sd[sd[\"final_class\"] == \"link\"]\n",
    "links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4602e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze_meaning_by_symbol(syms_data):\n",
    "    # convert scraped data to symbol: meaning data \n",
    "    data2 =[]\n",
    "    sym = \"\"\n",
    "    meanings = []\n",
    "\n",
    "    for i, r in syms_data.iterrows():\n",
    "        if r['final_class'] == \"sym\":\n",
    "            \n",
    "            data2.append({\"symbol\": sym, \"meaning\": meanings})\n",
    "            meanings = []\n",
    "            sym = r['text']\n",
    "        else:\n",
    "            if r['text'] is not None:\n",
    "                meanings.append(r['text'])\n",
    "        \n",
    "\n",
    "    data2 = pd.DataFrame(data2)\n",
    "    data2['n_meanings'] = data2.meaning.apply(len)\n",
    "    return data2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e2220",
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpointstring = \"animals being abused\"\n",
    "\n",
    "data2 = squeeze_meaning_by_symbol(sd)\n",
    "data3 = data2.explode('meaning')\n",
    "data3['mean_len'] = data3.meaning.str.len()\n",
    "data3.dropna(inplace=True)\n",
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff76dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = int(data3['mean_len'].sum())\n",
    "tstp = datetime.now().strftime(r\"%y.%m.%d-%H\")\n",
    "tc, tstp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0508c8",
   "metadata": {},
   "source": [
    "total chars extracted from c_all.html:  \n",
    "25.05.10 19:00 - 191953\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1833d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.to_csv(f\"datasets/c_scraped_{tstp}_{tc}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39063b3c",
   "metadata": {},
   "source": [
    "In the end, the above logic can be summarized in a function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11f6f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_letter(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        html = f.read()\n",
    "        \n",
    "    xp = \"/html/body/table[2]/tr/td/div/center/table/tr[4]/td[1]\"\n",
    "\n",
    "    tree = lhtml.fromstring(html)\n",
    "\n",
    "    #tree.text_content()\n",
    "\n",
    "    tabl = tree.xpath(xp)[0]\n",
    "\n",
    "    syms_data = collect_paragraphs(tabl)\n",
    "    syms_data = pd.DataFrame(syms_data)\n",
    "    data2 = squeeze_meaning_by_symbol(syms_data)\n",
    "    data3 = data2.explode('meaning')\n",
    "    data3['mean_len'] = data3.meaning.str.len()\n",
    "    data3.dropna(inplace=True)\n",
    "    return data3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f6b0c1",
   "metadata": {},
   "source": [
    "And executed on all the letters htmls to extract the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a0b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dfs = []\n",
    "save_dir = 'html/dreamdictionary'\n",
    "tstp = datetime.now().strftime(r\"%y.%m.%d-%H\")\n",
    "for f in os.listdir(save_dir):\n",
    "    if f.endswith(\".html\"):\n",
    "        try:\n",
    "            existing_df = extract_letter(os.path.join(save_dir, f))\n",
    "            existing_df[\"filename\"] = f\n",
    "            dfs.append(existing_df)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "dataset = pd.concat(dfs)\n",
    "dataset = dataset[dataset['mean_len'] > 0]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38d735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tstp = datetime.now().strftime(r\"%y.%m.%d-%H\")\n",
    "fname = f\"datasets/rescraped_{tstp}\"\n",
    "\n",
    "save_df_as_pretty_html(dataset, fname + \".html\")\n",
    "dataset.to_csv(fname + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8edf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset #[dataset['mean_len'] ==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e2e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(dataset.mean_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb7ec7",
   "metadata": {},
   "source": [
    "### Potential data augmentation\n",
    "the dream interpretations generally have a rather consistent structure. Maybe this can be leveraged to extract a more detailed dream symbol from the data, without running it through LLM. for further development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd9da8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential regex to split symbol and meaning\n",
    "\n",
    "\"(suggest|represent|symbolize|indicate|signif|mean|analogous|implies|denote|refers to)y?(ie)?s?ze? \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d97884",
   "metadata": {},
   "source": [
    "## Dreambank\n",
    "\n",
    "On this godforsaken website there is a section called dreambank, which has some dreams descriptions and interpretations of them by the moderators.  \n",
    "We'd like to use this as our test data, so we need to scrape them as well  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bec1852",
   "metadata": {},
   "source": [
    "### Scrape the list of dreams\n",
    "First, We need top scrape the dreambank.html page, which holds the links to all the interpreted dream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a80f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f\"html/dreambank/dreambank.html\"\n",
    "\n",
    "url = \"http://www.dreammoods.com/dreambank/\"\n",
    "download_page(url, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd220de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bank(file, xpath):\n",
    "    with open(file, \"r\") as f:\n",
    "        html = f.read()       \n",
    "\n",
    "    tree = lhtml.fromstring(html)\n",
    "\n",
    "    #tree.text_content()\n",
    "\n",
    "    tabl = tree.xpath(xpath)[0]\n",
    "\n",
    "    global syms_data\n",
    "    syms_data = []\n",
    "    r,c,t = breakdown(tabl)\n",
    "    syms_data = pd.DataFrame(syms_data)\n",
    "    data2 = squeeze_meaning_by_symbol(syms_data)\n",
    "    data3 = data2.explode('meaning')\n",
    "    data3['mean_len'] = data3.meaning.str.len()\n",
    "    return data3\n",
    "\n",
    "xps = [ \"/html/body/table[2]/tr/td/div/center/table/tr[3]/td[1]\" ,\n",
    "        \"/html/body/table[2]/tr/td/div/center/table/tr[3]/td[2]\",\n",
    "        \"/html/body/table[2]/tr/td/div/center/table/tr[3]/td[3]\"\n",
    "]\n",
    "sd = []\n",
    "for xp in xps:\n",
    "    extract_bank(file, xp)\n",
    "    sd.append(syms_data)\n",
    "\n",
    "df = pd.concat(sd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28ea75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeebaaa",
   "metadata": {},
   "source": [
    "Next, we extract the actual links and their description text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c675a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[ df.clas == \"link\"]\n",
    "df[['_', 'link', 'name']] = df['cont'].str.split(',', n=2, expand=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5e44be",
   "metadata": {},
   "source": [
    "The links contain a search term. \n",
    "We'll use this search term as filename to store the downloaded page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"srch\"] = df.link.str.split('search=', n=2, expand=True)[1].str.split('&', n=2, expand=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac1200",
   "metadata": {},
   "source": [
    "Some links do not conform to this structure, so we'll use their page name verbatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3478407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wat = df[df.srch.isna()].link.str.split(\"/\", n=5, expand=True)\n",
    "df.loc[df.srch.isna(),'srch'] = wat.loc[:,4]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eedbf4",
   "metadata": {},
   "source": [
    "Majority of those links do not work. By analyzing the url structure, we came up with an educated guess as to how they might be fixed. Some of it succeeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75126ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.link =  df.apply(lambda x: x.link.replace(\"dreambank\", x.srch[:-1] + \"s\") , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea5d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5036ff97",
   "metadata": {},
   "source": [
    "Then downloaded any pages that were available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42400d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "pref = \"html/dreambank/\"\n",
    "\n",
    "results = []\n",
    "for i, r in df.iterrows():\n",
    "    res = download_page(r['link'], pref + r['srch'] + r['name']  + \".html\")\n",
    "    results.append(res)\n",
    "    time.sleep(.2)  # Sleep for 1 second to avoid overwhelming the server\n",
    "\n",
    "df['res'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c9b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[ 'res', 'link', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e52ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['res','link'], ascending=True, inplace=True)\n",
    "df[[ 'res', 'link', 'name']].to_csv(\"dreamdic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d912bf",
   "metadata": {},
   "source": [
    "### Fixing html\n",
    "\n",
    "Being an ancient dinosaur poop that it is, the website is a horrendous case of html tag soup.  \n",
    "Even our highly customized extractor fails to properly parse the pages.  \n",
    "We'll summon modern web browser technology in the form of html5lib library to repair these pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527f358",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fix_html_file(input_filepath, output_filepath):\n",
    "    \"\"\"\n",
    "    Reads a broken HTML file, parses it using html5lib (like a browser),\n",
    "    and writes the corrected HTML to a new file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_filepath, 'r', encoding='utf-8') as f:\n",
    "            broken_html_content = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file not found at '{input_filepath}'\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file '{input_filepath}': {e}\")\n",
    "        return\n",
    "\n",
    "    # Parse the HTML using html5lib (browser-like parsing)\n",
    "    # This is where the \"fixing\" happens internally\n",
    "    soup = BeautifulSoup(broken_html_content, 'html5lib')\n",
    "\n",
    "    # Get the corrected HTML. .prettify() adds nice indentation.\n",
    "    # You can also use str(soup) for a less formatted output.\n",
    "    corrected_html_content = soup.prettify()\n",
    "\n",
    "    try:\n",
    "        with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(corrected_html_content)\n",
    "        print(f\"Successfully fixed HTML and saved to '{output_filepath}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing file '{output_filepath}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0a1166",
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_dir = 'html/dreambank'\n",
    "fixed_dir = 'html/dreambank/fixed'\n",
    "\n",
    "for f in os.listdir(broken_dir):\n",
    "    if f.endswith(\".html\"):\n",
    "        input_file = os.path.join(broken_dir, f)\n",
    "        output_file = os.path.join(fixed_dir, f)\n",
    "        fix_html_file(input_file, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45589f26",
   "metadata": {},
   "source": [
    "### Extracting from Dream Bank\n",
    "Once the pages are fixed, our scraper is again able to parse them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f4e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bank(file, xpath):\n",
    "    \n",
    "    with open(file, \"r\") as f:\n",
    "        html = f.read()\n",
    "        \n",
    "\n",
    "    tree = lhtml.fromstring(html)\n",
    "    #tree.text_content()\n",
    "\n",
    "    tabl = tree.xpath(xpath)[0]\n",
    "    \n",
    "    r,sd,c,t = breakdown(tabl)\n",
    "\n",
    "    \n",
    "    return r,sd,c,t\n",
    "    # syms_data = pd.DataFrame([c,t])\n",
    "    # # data2 = squeeze_meaning_by_symbol(syms_data)\n",
    "    # # data3 = data2.explode('meaning')\n",
    "    # # data3['mean_len'] = data3.meaning.str.len()\n",
    "    # return pd.DataFrame(), syms_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8638df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "sds = []\n",
    "save_dir = 'html/dreambank/fixed'\n",
    "\n",
    "#xpath = \"/html/body/table[2]/tr/td/div/center/table/tr[3]/td[1]\"\n",
    "xpath = \"/html/body/table[2]/tbody/tr/td/div/center/table/tbody/tr[3]/td[1]\"\n",
    "\n",
    "breakpointstring = \"I keep having this recurring\"\n",
    "\n",
    "for f in os.listdir(save_dir):\n",
    "    if f.endswith(\".html\"):\n",
    "        path = os.path.join(save_dir, f)\n",
    "        #print(path)\n",
    "        r,sd,c,t = extract_bank(path,xpath)\n",
    "        # data3[\"filename\"] = f\n",
    "        dfs.append(r)\n",
    "        sd = pd.DataFrame(sd)\n",
    "        #sd = squeeze_meaning_by_symbol(syms_data=sd)\n",
    "        sds.append(sd)\n",
    "#sds = sds)\n",
    "#sds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3564aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8537014",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_as_pretty_html(sds[0], 'example.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96862920",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\n",
    "with open(\"sandbox.txt\", \"w\") as f:\n",
    "    f.write(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd052d6a",
   "metadata": {},
   "source": [
    "# original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dfaa22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T18:21:01.708865Z",
     "iopub.status.busy": "2024-10-15T18:21:01.707831Z",
     "iopub.status.idle": "2024-10-15T18:21:01.714992Z",
     "shell.execute_reply": "2024-10-15T18:21:01.713800Z"
    },
    "papermill": {
     "duration": 0.014252,
     "end_time": "2024-10-15T18:21:01.717376",
     "exception": false,
     "start_time": "2024-10-15T18:21:01.703124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from IPython.display import FileLink, display\n",
    "\n",
    "def download_file(path, download_file_name):\n",
    "    os.chdir('/kaggle/working/')\n",
    "    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n",
    "    command = f\"zip {zip_name} {path} -r\"\n",
    "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        print(\"Unable to run zip command!\")\n",
    "        print(result.stderr)\n",
    "        return\n",
    "    display(FileLink(f'{download_file_name}.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc65b6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T18:21:01.726604Z",
     "iopub.status.busy": "2024-10-15T18:21:01.725718Z",
     "iopub.status.idle": "2024-10-15T18:21:01.768625Z",
     "shell.execute_reply": "2024-10-15T18:21:01.767508Z"
    },
    "papermill": {
     "duration": 0.050006,
     "end_time": "2024-10-15T18:21:01.770951",
     "exception": false,
     "start_time": "2024-10-15T18:21:01.720945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "download_file(\"/kaggle/working/dreams_interpretations.csv\", \"download\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5881987,
     "sourceId": 9634093,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.516826,
   "end_time": "2024-10-15T18:21:02.294568",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-15T18:20:42.777742",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
