{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709bb27208e23ba4",
   "metadata": {},
   "source": [
    "## **Step 1 - keywords Extraction**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af564d8f04df39",
   "metadata": {},
   "source": [
    "We have two datasets, one with dream text descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ecfbd7e24c7e53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T21:23:35.718332Z",
     "start_time": "2025-04-13T21:23:35.469996Z"
    }
   },
   "outputs": [],
   "source": [
    "from keyword_extractor import read_datasets, extract_and_save_keywords_from_dataframes\n",
    "from yaml_parser import load_config\n",
    "config = load_config()\n",
    "dream_df, keywords_df = read_datasets(config)\n",
    "dream_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c56a04aa56a02",
   "metadata": {},
   "source": [
    "And another one with interpretations of dreams according to keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c15465e0fca30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T21:23:35.755134Z",
     "start_time": "2025-04-13T21:23:35.747133Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd509c434a4e3b8d",
   "metadata": {},
   "source": [
    "Now, we will use pretrained LLMs in order to extract the given keywords from the keywords dataset , from the dream text description from the dream text dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c5d2fa9f32bfe5",
   "metadata": {},
   "source": [
    "### **GPT2**\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b7a0f6bf84fe5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T21:39:36.596114Z",
     "start_time": "2025-04-13T21:32:54.369789Z"
    }
   },
   "outputs": [],
   "source": [
    "dream_df = extract_and_save_keywords_from_dataframes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803344e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "css = \"\"\"\n",
    "    .table-style {\n",
    "                  width: 100%;\n",
    "                  border-style: solid;\n",
    "                  border-width: 5px;\n",
    "}\n",
    "\n",
    "    .table-style td {\n",
    "                  white-space:pre\n",
    "                  width: 100px;\n",
    "                  border-style: solid;\n",
    "                  border-width: 5px;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6dac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dream_df[[\"text_dream\",\"Dream Symbol\"]][:100].style\\\n",
    "  .set_table_attributes('class=\"table-style\"')\\\n",
    "  .to_html(\"datasets/dream_and_its_keys.html\", index=False, classes=css, border=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721b6935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31a7e75c",
   "metadata": {},
   "source": [
    "## Step 2 - Summarize interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4328b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandasql as ps\n",
    "import numpy as np\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65307ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dream_df= pd.read_csv('datasets/rsos_dream_data.tsv', sep='\\t')\n",
    "dream_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82be03f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_df = pd.read_csv(\"datasets/fixed_interpretations.csv\")\n",
    "keywords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "exmpl = dream_df[dream_df[\"text_dream\"].str.len()< 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cec8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "exmpl = exmpl[[\"text_dream\",\"Dream Symbol\"]].sample(20, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8ecf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exmpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f398e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = set(keywords_df[\"Dream Symbol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab995a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keys(dream):\n",
    "    text = dream[\"text_dream\"]\n",
    "    keys = dream[\"Dream Symbol\"].split(\",\")\n",
    "    return [k for k in keys if k.lower() in text.lower()]\n",
    "\n",
    "tst = exmpl.iloc[1]\n",
    "print(tst)\n",
    "keys = extract_keys(tst)[:10]\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e51e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exmpl[\"Dream Symbol\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3572d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "prmt = \"\"\"Given dream description, interpret the meaning of the dream. \n",
    "Provided also are the dream symbols that appear in the dream and their meanings. \n",
    "Use the dream symbols meanings to help you interpret the dream. \"\"\".replace(\"\\n\", \" \")\n",
    "\n",
    "rs = 42\n",
    "\n",
    "for i, ex in exmpl.iterrows():\n",
    "    print(ex)\n",
    "    keys = ex[\"Dream Symbol\"].split(\",\")[:5]\n",
    "    \n",
    "    #print(keys)\n",
    "    syms = keywords_df[keywords_df[\"Dream Symbol\"].isin(keys)]\n",
    "\n",
    "    descr = syms.apply(lambda r: f' - {r[\"Dream Symbol\"]}:  {r[\"Interpretation\"]}', axis = 1)\n",
    "    item = {\n",
    "        \"prompt\": prmt, \n",
    "        \"dream\": ex[\"text_dream\"],\n",
    "        \"symbols\": \"\\n\".join(descr),\n",
    "        }\n",
    "    dataset.append(item)\n",
    "    rs += 1\n",
    "    \n",
    "\n",
    "dataset = pd.DataFrame(dataset)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4431da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def release_all_gpu_memory(additional_objects=None):\n",
    "    import gc\n",
    "    import torch\n",
    "\n",
    "    # Delete model objects (make sure they're declared global or passed)\n",
    "    globals_to_clear = [\"model\", \"tokenizer\", \"text2text_generator\"] + additional_objects\n",
    "    for name in globals_to_clear:\n",
    "        if name in globals():\n",
    "            print(\"clearing \", name)\n",
    "            del globals()[name]\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"clearing cuda cache\")\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"clearing ipc cache\")\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "    print(\"✅ All GPU memory cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984fc120",
   "metadata": {},
   "outputs": [],
   "source": [
    "release_all_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e117bdf",
   "metadata": {},
   "source": [
    "### Summarize with flan-T5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb75fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "release_all_gpu_memory()\n",
    "\n",
    "# Step 1: Load FLAN-T5 model and tokenizer\n",
    "model_name = \"google/flan-t5-large\"\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Check model's max position embeddings\n",
    "print(f\"Model can handle up to {model.config} tokens.\")  # should be 1024\n",
    "\n",
    "\n",
    "text2text_generator = pipeline(\"text2text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        truncation=True,           # ✅ ensure truncation at tokenizer level\n",
    "        max_length=1024,           # ✅ allow longer input\n",
    "        device=device,\n",
    "        )\n",
    "\n",
    "\n",
    "# Step 2: Define input formatting\n",
    "def format_instruction(prompt, dream, symbols):\n",
    "    return (\n",
    "        f\"Instruction: {prompt.strip()}\\n\\n\"\n",
    "        f\"Dream: {dream.strip()}\\n\\n\"\n",
    "        f\"Symbols:\\n{symbols.strip()}\\n\\n\"\n",
    "        \"Interpretation:\"\n",
    "    )\n",
    "\n",
    "# Step 3: Batch interpret function\n",
    "def batch_interpret_df(df, model_pipeline, batch_size=4, max_output_length=250):\n",
    "    interpretations = []\n",
    "    for i in tqdm(range(0, len(df), batch_size), desc=\"Generating Interpretations\"):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        inputs = [\n",
    "            format_instruction(row[\"prompt\"], row[\"dream\"], row[\"symbols\"])\n",
    "            for _, row in batch_df.iterrows()\n",
    "        ]\n",
    "        print(len(inputs[0]))\n",
    "        outputs = model_pipeline(inputs, max_length=max_output_length, do_sample=False)\n",
    "        interpretations.extend([out[\"generated_text\"] for out in outputs])\n",
    "    df[\"interpretation\"] = interpretations\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95cd587",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b2a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = batch_interpret_df(dataset, text2text_generator, batch_size=1)\n",
    "# print(result_df[[\"dream\", \"interpretation\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b435acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019cae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3555e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[['prompt', 'symbols','dream', 'interpretation']].to_html(\"datasets/dream_interpretations.html\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78ec2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.interpretation.str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da433d4",
   "metadata": {},
   "source": [
    "### Summarize with Mistral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3c384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 Script: dream_interpreter_mistral_4bit.py\n",
    "# ------------------------------------------\n",
    "# This script loads the Mistral-7B-Instruct model in 4-bit quantized mode\n",
    "# and runs batch dream interpretation using instruction prompting.\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Format prompt for causal model\n",
    "\n",
    "def format_mistral_input(prompt, dream, symbols):\n",
    "    return f\"\"\"### Instruction:\n",
    "{prompt.strip()}\n",
    "\n",
    "### Dream:\n",
    "{dream.strip()}\n",
    "\n",
    "### Symbols:\n",
    "{symbols.strip()}\n",
    "\n",
    "### Interpretation:\"\"\"\n",
    "\n",
    "# Load Mistral 7B in 4-bit mode\n",
    "def load_mistral_4bit_pipeline(model_name=\"mistralai/Mistral-7B-Instruct\", max_new_tokens=256):\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    return pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "# Batch processing\n",
    "def batch_generate_interpretations(df, model_pipeline, batch_size=2, max_input_tokens=8192):\n",
    "    interpretations = []\n",
    "    for i in tqdm(range(0, len(df), batch_size), desc=\"Generating Interpretations\"):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "\n",
    "        prompts = [format_mistral_input(row[\"prompt\"], row[\"dream\"], row[\"symbols\"]) for _, row in batch_df.iterrows()]\n",
    "\n",
    "        for prompt in prompts:\n",
    "            token_count = len(model_pipeline.tokenizer.encode(prompt))\n",
    "            if token_count > max_input_tokens:\n",
    "                print(f\"⚠️ Prompt truncated: {token_count} tokens (limit = {max_input_tokens})\")\n",
    "\n",
    "        outputs = model_pipeline(prompts)\n",
    "        batch_outputs = [out[0][\"generated_text\"].split(\"### Interpretation:\")[-1].strip() for out in outputs]\n",
    "\n",
    "        interpretations.extend(batch_outputs)\n",
    "\n",
    "    df[\"interpretation\"] = interpretations\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50747c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0263914",
   "metadata": {},
   "outputs": [],
   "source": [
    "release_all_gpu_memory([\"model_pipeline\"])\n",
    "print(\"\\n🔁 Loading Mistral-7B-Instruct in 4-bit...\")\n",
    "model_pipeline = load_mistral_4bit_pipeline(\"mistralai/Mistral-7B-Instruct-v0.2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff5df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n🧠 Running interpretations...\")\n",
    "\n",
    "result_df = batch_generate_interpretations(dataset, model_pipeline, batch_size=4)\n",
    "print(result_df[[\"dream\", \"interpretation\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4fa381",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e053795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_as_pretty_html(df, filename=\"output.html\"):\n",
    "    # Convert newlines to <br> for HTML\n",
    "    df_html_ready = df.copy()\n",
    "    for col in df_html_ready.columns:\n",
    "        df_html_ready[col] = df_html_ready[col].astype(str).str.replace('\\n', '<br>', regex=False)\n",
    "\n",
    "    # Generate styled HTML\n",
    "    html = df_html_ready.to_html(\n",
    "        escape=False,  # Needed to render <br>\n",
    "        index=False,\n",
    "        border=0,\n",
    "        classes=\"styled-table\"\n",
    "    )\n",
    "\n",
    "    # Add CSS styling\n",
    "    style = \"\"\"\n",
    "    <style>\n",
    "    .styled-table {\n",
    "        border-collapse: collapse;\n",
    "        margin: 25px 0;\n",
    "        font-size: 16px;\n",
    "        font-family: Arial, sans-serif;\n",
    "        width: 100%;\n",
    "        table-layout: auto;\n",
    "        word-break: break-word;\n",
    "    }\n",
    "    .styled-table th, .styled-table td {\n",
    "        border: 1px solid #dddddd;\n",
    "        padding: 10px;\n",
    "        vertical-align: top;\n",
    "        text-align: left;\n",
    "    }\n",
    "    .styled-table th {\n",
    "        background-color: #f2f2f2;\n",
    "    }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "\n",
    "    # Write full HTML document\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"<!DOCTYPE html><html><head>{style}</head><body>{html}</body></html>\")\n",
    "\n",
    "    print(f\"✅ HTML table saved to: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a4d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[\"symbols\"] = result_df[\"symbols\"].str.replace(r\"\\n\", \"<br>\")\n",
    "save_df = result_df[['prompt', 'symbols','dream', 'interpretation']]\n",
    "path = \"datasets/mistral_interpretations.html\"\n",
    "save_df_as_pretty_html(save_df, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4ec161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dream",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
