{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709bb27208e23ba4",
   "metadata": {},
   "source": [
    "## **Step 1 - keywords Extraction**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af564d8f04df39",
   "metadata": {},
   "source": [
    "We have two datasets, one with dream text descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ecfbd7e24c7e53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T21:23:35.718332Z",
     "start_time": "2025-04-13T21:23:35.469996Z"
    }
   },
   "outputs": [],
   "source": [
    "from keyword_extractor import read_datasets, extract_and_save_keywords_from_dataframes\n",
    "from yaml_parser import load_config\n",
    "config = load_config()\n",
    "dream_df, keywords_df = read_datasets(config)\n",
    "dream_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c56a04aa56a02",
   "metadata": {},
   "source": [
    "And another one with interpretations of dreams according to keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c15465e0fca30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T21:23:35.755134Z",
     "start_time": "2025-04-13T21:23:35.747133Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd509c434a4e3b8d",
   "metadata": {},
   "source": [
    "Now, we will use pretrained LLMs in order to extract the given keywords from the keywords dataset , from the dream text description from the dream text dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c5d2fa9f32bfe5",
   "metadata": {},
   "source": [
    "### **GPT2**\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b7a0f6bf84fe5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T21:39:36.596114Z",
     "start_time": "2025-04-13T21:32:54.369789Z"
    }
   },
   "outputs": [],
   "source": [
    "dream_df = extract_and_save_keywords_from_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a7e75c",
   "metadata": {},
   "source": [
    "## Step 2 - Summarize interpretations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896dc1f6",
   "metadata": {},
   "source": [
    "### Load data and prepare (small) dataset for experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4328b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandasql as ps\n",
    "from plotly import express as px\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "from utils import  release_all_gpu_memory, save_df_as_pretty_html\n",
    "from summarizer import load_causal_model, batch_generate_interpretations\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3572d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "prmt = \"\"\"Given dream description, interpret the meaning of the dream. \n",
    "Provided also are the dream symbols that appear in the dream and their meanings. \n",
    "Use the dream symbols meanings to help you interpret the dream. \"\"\".replace(\"\\n\", \" \")\n",
    "\n",
    "\n",
    "for i, ex in dream_df.iterrows():\n",
    "    #print(ex)\n",
    "    keys = ex[\"Dream Symbol\"].split(\",\")[:5]\n",
    "    \n",
    "    #print(keys)\n",
    "    syms = keywords_df[keywords_df[\"Dream Symbol\"].isin(keys)]\n",
    "\n",
    "    descr = syms.apply(lambda r: f' - {r[\"Dream Symbol\"]}:  {r[\"Interpretation\"]}', axis = 1)\n",
    "    item = {\n",
    "        \"prompt\": prmt, \n",
    "        \"dream\": ex[\"text_dream\"],\n",
    "        \"symbols\": \"\\n\".join(descr),\n",
    "        }\n",
    "    dataset.append(item)\n",
    "    \n",
    "\n",
    "dataset = pd.DataFrame(dataset)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e117bdf",
   "metadata": {},
   "source": [
    "### Summarize with flan-T5-large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb75fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "release_all_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16073714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load FLAN-T5 model and tokenizer\n",
    "model_name = \"google/flan-t5-large\"\n",
    "model_name_short = model_name.split(\"/\")[-1]\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "model, tokenizer = load_causal_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034d08aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2text_generator = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=1024,           # ✅ allow longer input\n",
    "        truncation=True,           # ✅ ensure truncation at tokenizer level\n",
    "        device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b2a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tstp = datetime.now().strftime(r\"%y.%m.%d-%H\")\n",
    "result_df = batch_generate_interpretations(dataset, text2text_generator, batch_size=100, max_length=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b868a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "postproc = lambda out: out[\"generated_text\"].strip()\n",
    "result_df[\"interpretation\"] = result_df[\"interpretation\"].apply(postproc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b435acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019cae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a0964",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df = result_df[['prompt', 'symbols','dream', 'interpretation']]\n",
    "\n",
    "path = f\"output/{model_name_short}_{tstp}\"\n",
    "\n",
    "save_df_as_pretty_html(save_df, path + \".html\")\n",
    "\n",
    "save_df.to_csv(path + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78ec2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.interpretation.str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da433d4",
   "metadata": {},
   "source": [
    "### Summarize with Mistral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import load_mistral_4bit_model, find_max_batch_size, PromptFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272ff4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Mistral-7B-Instruct in 4-bit...\")\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model_name_short = model_name.split(\"/\")[-1]\n",
    "\n",
    "model_family=\"decoder\"\n",
    "formatter = PromptFormatter(model_family)\n",
    "# max_new_tokens=256         max_new_tokens=max_new_tokens,\n",
    "\n",
    "model, tokenizer = load_mistral_4bit_model(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e867a",
   "metadata": {},
   "source": [
    "#### Prepare and analize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930fb752",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"input\"] = dataset.apply(lambda r: formatter.format(r['prompt'], r['dream'], r['symbols']), axis = 1)\n",
    "dataset[\"len\"] = dataset[\"input\"].str.len()\n",
    "dataset[\"input_tokens\"] = dataset.input.apply(lambda prmt: tokenizer.tokenize(prmt, truncation=False, max_length=1024))\n",
    "dataset[\"input_tokens_len\"] = dataset.input_tokens.apply(len)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9122acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(dataset, x = \"input_tokens_len\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e456c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(dataset, x = \"len\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e02a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(dataset, x = \"len\", y = \"input_tokens_len\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea0ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[dataset[\"input_tokens_len\"] <1024]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db76ed77",
   "metadata": {},
   "source": [
    "### Run the pipeline on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683039df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.to_csv(\"datasets/prepared_dataset.csv\", index=False)\n",
    "dataset = pd.read_csv(\"datasets/prepared_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc28d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt = dataset.iloc[3248]\n",
    "print(f\"Sample prompt length (characters): {len(sample_prompt)}\")\n",
    "optimal_batch_size = find_max_batch_size(model, tokenizer, sample_prompt,   max_possible=2048,     max_length=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd3f13",
   "metadata": {},
   "source": [
    "first run on 24 Gb GPU: \n",
    "```\n",
    "Sample prompt length (characters): 2437  \n",
    "Trying batch_size = 512... ❌ OOM  \n",
    "Trying batch_size = 256... ❌ OOM  \n",
    "Trying batch_size = 128... ❌ OOM  \n",
    "Trying batch_size = 64... ❌ OOM  \n",
    "Trying batch_size = 32... ✅ success  \n",
    "Trying batch_size = 48... ✅ success  \n",
    "Trying batch_size = 56... ✅ success  \n",
    "Trying batch_size = 60... ❌ OOM  \n",
    "Trying batch_size = 58... ❌ OOM  \n",
    "Trying batch_size = 57... ✅ success  \n",
    "\n",
    "✅ Optimal batch size: 57\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d9e780",
   "metadata": {},
   "source": [
    "second run on 24 Gb GPU: \n",
    "```\n",
    "Sample prompt length (characters): 2437\n",
    "Trying batch_size = 1024... ❌ OOM\n",
    "Trying batch_size = 512... ❌ OOM\n",
    "Trying batch_size = 256... ❌ OOM\n",
    "Trying batch_size = 128... ❌ OOM\n",
    "Trying batch_size = 64... ✅ success\n",
    "Trying batch_size = 96... ✅ success\n",
    "Trying batch_size = 112... ❌ OOM\n",
    "Trying batch_size = 104... ❌ OOM\n",
    "Trying batch_size = 100... ❌ OOM\n",
    "Trying batch_size = 98... ✅ success\n",
    "Trying batch_size = 99... ❌ OOM\n",
    "\n",
    "✅ Optimal batch size: 98\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02089655",
   "metadata": {},
   "outputs": [],
   "source": [
    "release_all_gpu_memory([\"model\",\"model_pipeline\",\"dataloader\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7376bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = Dataset.from_pandas(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b81c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_batch_size = 96\n",
    "\n",
    "model_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        batch_size=optimal_batch_size,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length = 1024,\n",
    "        truncation=False,\n",
    "        do_sample=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2847e81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import summarizer\n",
    "reload(summarizer)\n",
    "\n",
    "# Now re-import manually\n",
    "from summarizer import batch_generate_interpretations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff5df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n🧠 Running interpretations...\")\n",
    "tstp = datetime.now().strftime(r\"%y.%m.%d-%H\")\n",
    "\n",
    "result_df = batch_generate_interpretations(dataloader, model_pipeline, formatter, batch_size=optimal_batch_size, max_length=1024)\n",
    "#print(result_df[[\"dream\", \"interpretation\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a4d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_df = result_df[['prompt', 'symbols','dream', 'interpretation']]\n",
    "\n",
    "path = f\"output/{model_name_short}_{tstp}\"\n",
    "save_df_as_pretty_html(save_df, path + \".html\")\n",
    "\n",
    "save_df.to_csv(path + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a0690",
   "metadata": {},
   "source": [
    "# Memory investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0f6439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import globals_snapshot\n",
    "\n",
    "tps = globals_snapshot()\n",
    "\n",
    "q = \"\"\"\n",
    "select type, count(var) as cnt\n",
    "from tps\n",
    "group by type \n",
    "order by cnt desc \n",
    "\n",
    "\"\"\"\n",
    "df = ps.sqldf(q)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
