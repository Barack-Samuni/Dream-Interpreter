{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709bb27208e23ba4",
   "metadata": {},
   "source": [
    "## **Step 1 - keywords Extraction**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af564d8f04df39",
   "metadata": {},
   "source": [
    "We have two datasets, one with dream text descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ecfbd7e24c7e53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:35:41.288127Z",
     "start_time": "2025-05-03T16:35:30.582438Z"
    }
   },
   "outputs": [],
   "source": [
    "from keyword_extractor import read_datasets, extract_and_save_keywords_from_dataframes\n",
    "from yaml_parser import load_config\n",
    "config = load_config()\n",
    "dream_df, keywords_df = read_datasets(config)\n",
    "dream_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c56a04aa56a02",
   "metadata": {},
   "source": [
    "And another one with interpretations of dreams according to keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c15465e0fca30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:35:42.053745Z",
     "start_time": "2025-05-03T16:35:42.043610Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd509c434a4e3b8d",
   "metadata": {},
   "source": [
    "Then, we used a pretrained Sentence transformer to encode the dream embeddings and keyword embeddings and try to extract the most significant keywords from each dream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c5d2fa9f32bfe5",
   "metadata": {},
   "source": [
    "### **all-MiniLM-L6-v2**\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b7a0f6bf84fe5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:38:32.546001Z",
     "start_time": "2025-05-03T16:35:42.137333Z"
    }
   },
   "outputs": [],
   "source": [
    "dream_df = extract_and_save_keywords_from_dataframes()\n",
    "dream_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58dd5943a0d1a8",
   "metadata": {},
   "source": [
    "To view the dataframe better, We will filter out the interesting columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e6946480217fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:38:32.645080Z",
     "start_time": "2025-05-03T16:38:32.638181Z"
    }
   },
   "outputs": [],
   "source": [
    "columns_to_show = ['text_dream', 'Dream Symbol']\n",
    "dream_df[columns_to_show]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a7e75c",
   "metadata": {},
   "source": [
    "## Step 2 - Summarize interpretations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d4023e12db1af",
   "metadata": {},
   "source": [
    "After extracting the meaningful keywords, we tried to fetch the matching interpretation for each extracted keyword and use a pretrained LLM to summarize these interpretations into one interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896dc1f6",
   "metadata": {},
   "source": [
    "### Load data and prepare (small) dataset for experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4328b3ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:38:32.861969Z",
     "start_time": "2025-05-03T16:38:32.769750Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandasql as ps\n",
    "from plotly import express as px\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "from utils import  release_all_gpu_memory, save_df_as_pretty_html\n",
    "from summarizer import load_causal_model, batch_generate_interpretations\n",
    "import torch\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e3d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.to_csv(\"datasets/prepared_dataset.csv\", index=False)\n",
    "dataset = pd.read_csv(\"datasets/prepared_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1018cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset.sort_values(\"input_tokens_len\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fc164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter dataset only to inputs of size that we can handle\n",
    "dataset = dataset[dataset[\"input_tokens_len\"] <1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9847ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:38:33.690634Z",
     "start_time": "2025-05-03T16:38:33.684338Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6893ed4a4144bd",
   "metadata": {},
   "source": [
    "Now, we will create a prompt for the LLM. The prompt will include a request for the LLM to summarize the interpretations. It will get the dream description, the keywords, and the interpretations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e117bdf",
   "metadata": {},
   "source": [
    "### Summarize with flan-T5-large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb75fd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:38:34.232059Z",
     "start_time": "2025-05-03T16:38:34.043026Z"
    }
   },
   "outputs": [],
   "source": [
    "release_all_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16073714",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:38:35.957547Z",
     "start_time": "2025-05-03T16:38:34.274307Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Load FLAN-T5 model and tokenizer\n",
    "model_name = \"google/flan-t5-large\"\n",
    "model_name_short = model_name.split(\"/\")[-1]\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "model, tokenizer = load_causal_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034d08aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:38:42.022208Z",
     "start_time": "2025-05-03T16:38:35.994449Z"
    }
   },
   "outputs": [],
   "source": [
    "text2text_generator = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=1024,           # âœ… allow longer input\n",
    "        truncation=True,           # âœ… ensure truncation at tokenizer level\n",
    "        device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb00ba2d4b78df0e",
   "metadata": {},
   "source": [
    "Create interpretations in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b2a8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:39:00.197400Z",
     "start_time": "2025-05-03T16:38:42.151455Z"
    }
   },
   "outputs": [],
   "source": [
    "tstp = datetime.now().strftime(r\"%y.%m.%d-%H\")\n",
    "result_df = batch_generate_interpretations(dataset, text2text_generator, batch_size=100, max_length=250)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b868a34a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:39:00.264108Z",
     "start_time": "2025-05-03T16:39:00.258724Z"
    }
   },
   "outputs": [],
   "source": [
    "postproc = lambda out: out[\"generated_text\"].strip()\n",
    "result_df[\"interpretation\"] = result_df[\"interpretation\"].apply(postproc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b435acdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:39:00.314631Z",
     "start_time": "2025-05-03T16:39:00.304180Z"
    }
   },
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019cae1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:39:00.390995Z",
     "start_time": "2025-05-03T16:39:00.384987Z"
    }
   },
   "outputs": [],
   "source": [
    "result_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac4b6293edbffe0",
   "metadata": {},
   "source": [
    "We saw that the interpretations are not quite good, and not that related to the dream description. We tried to save the dataframe for further research and saw that the problem applies to many cells and tried another model called Mistral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a0964",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:39:00.487159Z",
     "start_time": "2025-05-03T16:39:00.482158Z"
    }
   },
   "outputs": [],
   "source": [
    "save_df = result_df[['prompt', 'symbols','dream', 'interpretation']]\n",
    "\n",
    "path = f\"output/{model_name_short}_{tstp}\"\n",
    "\n",
    "save_df_as_pretty_html(save_df, path + \".html\")\n",
    "\n",
    "save_df.to_csv(path + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78ec2c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:39:00.543634Z",
     "start_time": "2025-05-03T16:39:00.527153Z"
    }
   },
   "outputs": [],
   "source": [
    "result_df.interpretation.str.len()  # TODO: Remember why we sorted interpretations by length..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da433d4",
   "metadata": {},
   "source": [
    "### Summarize with Mistral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272ff4c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T16:41:49.322476Z",
     "start_time": "2025-05-03T16:39:01.241846Z"
    }
   },
   "outputs": [],
   "source": [
    "from summarizer import load_mistral_4bit_model, find_max_batch_size\n",
    "print(\"Loading Mistral-7B-Instruct in 4-bit...\")\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model_name_short = model_name.split(\"/\")[-1]\n",
    "\n",
    "model_family=\"decoder\"\n",
    "\n",
    "# max_new_tokens=256         max_new_tokens=max_new_tokens,\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b03e29",
   "metadata": {},
   "source": [
    "#### measure maximum batch size a given machine can hadle without OOMing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6504c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt = dataset.input[0]\n",
    "print(f\"Sample prompt length (characters): {len(sample_prompt)}\")\n",
    "model, tokenizer = load_mistral_4bit_model(model_name)\n",
    "optimal_batch_size = find_max_batch_size(model, tokenizer, sample_prompt, max_possible=2048, max_new_tokens=512, ) # TODO: measure with max_new_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd3f13",
   "metadata": {},
   "source": [
    "first run on 24 Gb GPU: \n",
    "```\n",
    "Sample prompt length (characters): 2437  \n",
    "Trying batch_size = 512... âŒ OOM  \n",
    "Trying batch_size = 256... âŒ OOM  \n",
    "Trying batch_size = 128... âŒ OOM  \n",
    "Trying batch_size = 64... âŒ OOM  \n",
    "Trying batch_size = 32... âœ… success  \n",
    "Trying batch_size = 48... âœ… success  \n",
    "Trying batch_size = 56... âœ… success  \n",
    "Trying batch_size = 60... âŒ OOM  \n",
    "Trying batch_size = 58... âŒ OOM  \n",
    "Trying batch_size = 57... âœ… success  \n",
    "\n",
    "âœ… Optimal batch size: 57\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d9e780",
   "metadata": {},
   "source": [
    "second run on 24 Gb GPU: \n",
    "```\n",
    "Sample prompt length (characters): 2437\n",
    "Trying batch_size = 1024... âŒ OOM\n",
    "Trying batch_size = 512... âŒ OOM\n",
    "Trying batch_size = 256... âŒ OOM\n",
    "Trying batch_size = 128... âŒ OOM\n",
    "Trying batch_size = 64... âœ… success\n",
    "Trying batch_size = 96... âœ… success\n",
    "Trying batch_size = 112... âŒ OOM\n",
    "Trying batch_size = 104... âŒ OOM\n",
    "Trying batch_size = 100... âŒ OOM\n",
    "Trying batch_size = 98... âœ… success\n",
    "Trying batch_size = 99... âŒ OOM\n",
    "\n",
    "âœ… Optimal batch size: 98\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02089655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory after benchmarking\n",
    "release_all_gpu_memory([\"model\",\"model_pipeline\",\"dataloader\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d986c361",
   "metadata": {},
   "source": [
    "### Run the pipeline on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8201619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b81c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_batch_size = 40\n",
    "\n",
    "def build_pipeline(model_name, batch_size, ):\n",
    "    model, tokenizer = load_mistral_4bit_model(model_name)\n",
    "    model_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            batch_size=batch_size,\n",
    "            tokenizer=tokenizer,\n",
    "            #max_length=1024,  \n",
    "            max_new_tokens=512,         # âœ… controls output size\n",
    "            truncation=True,            # âœ… safely truncates long inputs\n",
    "            return_full_text=False,     # âœ… excludes input from output\n",
    "            do_sample=False\n",
    "        )\n",
    "    return model_pipeline\n",
    "\n",
    "model_pipeline = build_pipeline(model_name, optimal_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2847e81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to re-import a function in case you've updated it's code in summarizer.py\n",
    "from importlib import reload\n",
    "import summarizer\n",
    "reload(summarizer)\n",
    "\n",
    "# Now re-import manually\n",
    "from summarizer import batch_generate_interpretations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff5df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nðŸ§  Running interpretations...\")\n",
    "tstp = datetime.now().strftime(r\"%y.%m.%d-%H\")\n",
    "\n",
    "batch_generate_interpretations(dataset, model_pipeline, save_dir=\"output\",\n",
    "                               input_column=\"input\", output_column=\"interpretation\", \n",
    "                               batch_size=optimal_batch_size)\n",
    "#print(result_df[[\"dream\", \"interpretation\"]])\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a0690",
   "metadata": {},
   "source": [
    "# Memory investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0f6439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import globals_snapshot\n",
    "\n",
    "tps = globals_snapshot()\n",
    "\n",
    "q = \"\"\"\n",
    "select type, count(var) as cnt\n",
    "from tps\n",
    "group by type \n",
    "order by cnt desc \n",
    "\n",
    "\"\"\"\n",
    "df = ps.sqldf(q)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bbf3d0879e878",
   "metadata": {},
   "source": [
    "It didn't seem to help... So we then tried to improve our keyword extraction using:\n",
    "1. First - semantic search to narrow down the search of the keywords to only the semantically close ones.\n",
    "2. Second - MMR (Maximal Marginal Relevance) to increase the diversity of keywords extracted from the dream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54edc9d7f6bbc9",
   "metadata": {},
   "source": [
    "#TODO: Summarize better_keywords_extraction.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f81a5af772a13",
   "metadata": {},
   "source": [
    "## **Evaluation**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa317c15fd3c5bd3",
   "metadata": {},
   "source": [
    "We evaluated the performance of the dream interpretation using BLEU,perplexity,ROUGE, and BERT. **It's important to mention: evaluation was tested on a small sample of 5 rows, and also the dream interpretation was compared to the dream itself and that might be the reason for the small values of the metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc4b89ecbcc5a7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T17:06:39.432583Z",
     "start_time": "2025-05-03T17:06:34.660817Z"
    }
   },
   "outputs": [],
   "source": [
    "from evaluation import evaluate_dream_interpretations\n",
    "import pandas as pd\n",
    "dreams_interpretations_df = pd.read_csv('datasets/Mistral-7B-Instruct-v0.2_25.04.17-16.csv')\n",
    "dreams_interpretations_df = evaluate_dream_interpretations(dreams_interpretations_df)\n",
    "dreams_interpretations_df.to_csv('datasets/Mistral-7B-Instruct-v0.2_25.04.17-16_evaluated.csv', index=False)\n",
    "dreams_interpretations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d13ba9772d675f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T17:11:52.535627Z",
     "start_time": "2025-05-03T17:11:51.989396Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Create figure with subplots for non-perplexity scores\n",
    "fig1, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "plt.grid()\n",
    "\n",
    "# Plot distributions without perplexity\n",
    "scores_without_perplexity = ['BLEU', 'ROUGE', 'BERT']\n",
    "for score in scores_without_perplexity:\n",
    "    sns.kdeplot(data=dreams_interpretations_df[score], label=score, ax=ax1)\n",
    "ax1.set_title('Score Distributions (BLEU, ROUGE, BERT)')\n",
    "ax1.legend()\n",
    "\n",
    "# Calculate statistics for heatmap without perplexity\n",
    "stats_df = pd.DataFrame()\n",
    "for score in scores_without_perplexity:\n",
    "    stats_df[score] = [\n",
    "        dreams_interpretations_df[score].min(),\n",
    "        dreams_interpretations_df[score].max(),\n",
    "        dreams_interpretations_df[score].mean(),\n",
    "        dreams_interpretations_df[score].median(),\n",
    "        stats.mode(dreams_interpretations_df[score])[0]\n",
    "    ]\n",
    "stats_df.index = ['Min', 'Max', 'Average', 'Median', 'Mode']\n",
    "\n",
    "# Plot heatmap\n",
    "sns.heatmap(stats_df, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax2)\n",
    "ax2.set_title('Score Statistics (BLEU, ROUGE, BERT)')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Create separate figure for perplexity\n",
    "fig2, (ax3, ax4) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Plot perplexity distribution\n",
    "sns.kdeplot(data=dreams_interpretations_df['perplexity'], ax=ax3)\n",
    "ax3.set_title('Perplexity Distribution')\n",
    "\n",
    "# Calculate perplexity statistics\n",
    "perplexity_stats = pd.DataFrame({\n",
    "    'perplexity': [\n",
    "        dreams_interpretations_df['perplexity'].min(),\n",
    "        dreams_interpretations_df['perplexity'].max(),\n",
    "        dreams_interpretations_df['perplexity'].mean(),\n",
    "        dreams_interpretations_df['perplexity'].median(),\n",
    "        stats.mode(dreams_interpretations_df['perplexity'])[0]\n",
    "    ]\n",
    "})\n",
    "perplexity_stats.index = ['Min', 'Max', 'Average', 'Median', 'Mode']\n",
    "\n",
    "# Plot perplexity heatmap\n",
    "sns.heatmap(perplexity_stats, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax4)\n",
    "ax4.set_title('Perplexity Statistics')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dbda02eb4d1cd5",
   "metadata": {},
   "source": [
    "We can draw the following conclusions:\n",
    "1. The bleu score is incredibly low (a good result should be 20-40, we didn't even get 1...). This means that there is a weak overlap between the dream and its interpretation.\n",
    "2. Same for the Rouge.\n",
    "3. BERT averages at 0.6, which is not that bad considering that a good value is 0.85â€“0.9 that indicates some semantic similarity between the dream and its interpretation.\n",
    "4. perplexity is terrible since a good value is under 20...\n",
    "5. We interpret the results using this table:\n",
    "| **Metric** | **High Score Meaning** | **Low Score Meaning** | **Preferred Score** | **Typical Values for Good Results** | **Why?** |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| **BLEU** | High n-gram overlap between reference and candidate text | Low n-gram overlap between reference and candidate text | **High** | 20â€“40 (moderate), 40+ (good) | High BLEU indicates the candidate text closely matches the reference text. |\n",
    "| **Perplexity** | Candidate text is unpredictable and diverges from reference distribution | Candidate text is predictable, fluent, and aligned with reference distribution | **Low** | < 20 (for good results) | Low perplexity shows that the candidate text is fluent, consistent, and aligned with the reference. |\n",
    "| **ROUGE** | More overlapping n-grams (e.g., unigrams, bigrams) and higher recall of key phrases | Fewer overlapping n-grams and poor recall of key phrases | **High** | 30â€“50 (good), 50+ (very good) | High ROUGE suggests greater similarity between the candidate and reference texts. |\n",
    "| **BERTScore** | Strong semantic similarity between the candidate and reference text | Weak semantic similarity between the candidate and reference text | **High** | 0.85â€“0.98 (good) | Higher BERTScore reflects that the candidate preserves the meaning of the reference text. |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
