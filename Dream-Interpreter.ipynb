{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709bb27208e23ba4",
   "metadata": {},
   "source": [
    "## **Step 1 - keywords Extraction**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af564d8f04df39",
   "metadata": {},
   "source": [
    "We have two datasets, one with dream text descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ecfbd7e24c7e53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T21:23:35.718332Z",
     "start_time": "2025-04-13T21:23:35.469996Z"
    }
   },
   "outputs": [],
   "source": [
    "from keyword_extractor import read_datasets, extract_and_save_keywords_from_dataframes\n",
    "from yaml_parser import load_config\n",
    "config = load_config()\n",
    "dream_df, keywords_df = read_datasets(config)\n",
    "dream_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c56a04aa56a02",
   "metadata": {},
   "source": [
    "And another one with interpretations of dreams according to keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c15465e0fca30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T21:23:35.755134Z",
     "start_time": "2025-04-13T21:23:35.747133Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd509c434a4e3b8d",
   "metadata": {},
   "source": [
    "Now, we will use pretrained LLMs in order to extract the given keywords from the keywords dataset , from the dream text description from the dream text dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c5d2fa9f32bfe5",
   "metadata": {},
   "source": [
    "### **GPT2**\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b7a0f6bf84fe5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T21:39:36.596114Z",
     "start_time": "2025-04-13T21:32:54.369789Z"
    }
   },
   "outputs": [],
   "source": [
    "dream_df = extract_and_save_keywords_from_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a7e75c",
   "metadata": {},
   "source": [
    "## Step 2 - Summarize interpretations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896dc1f6",
   "metadata": {},
   "source": [
    "### Load data and prepare (small) dataset for experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4328b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandasql as ps\n",
    "from plotly import express as px\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "from utils import  release_all_gpu_memory, save_df_as_pretty_html\n",
    "from summarizer import load_causal_model, batch_generate_interpretations\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e3d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.to_csv(\"datasets/prepared_dataset.csv\", index=False)\n",
    "dataset = pd.read_csv(\"datasets/prepared_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1018cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset.sort_values(\"input_tokens_len\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fc164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter dataset only to inputs of size that we can handle\n",
    "dataset = dataset[dataset[\"input_tokens_len\"] <1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9847ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e117bdf",
   "metadata": {},
   "source": [
    "### Summarize with flan-T5-large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb75fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "release_all_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16073714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load FLAN-T5 model and tokenizer\n",
    "model_name = \"google/flan-t5-large\"\n",
    "model_name_short = model_name.split(\"/\")[-1]\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "model, tokenizer = load_causal_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034d08aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2text_generator = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=1024,           # ✅ allow longer input\n",
    "        truncation=True,           # ✅ ensure truncation at tokenizer level\n",
    "        device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b2a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tstp = datetime.now().strftime(r\"%y.%m.%d-%H\")\n",
    "result_df = batch_generate_interpretations(dataset, text2text_generator, batch_size=100, max_length=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b868a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "postproc = lambda out: out[\"generated_text\"].strip()\n",
    "result_df[\"interpretation\"] = result_df[\"interpretation\"].apply(postproc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b435acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019cae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a0964",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df = result_df[['prompt', 'symbols','dream', 'interpretation']]\n",
    "\n",
    "path = f\"output/{model_name_short}_{tstp}\"\n",
    "\n",
    "save_df_as_pretty_html(save_df, path + \".html\")\n",
    "\n",
    "save_df.to_csv(path + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78ec2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.interpretation.str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da433d4",
   "metadata": {},
   "source": [
    "### Summarize with Mistral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272ff4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import load_mistral_4bit_model, find_max_batch_size\n",
    "print(\"Loading Mistral-7B-Instruct in 4-bit...\")\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model_name_short = model_name.split(\"/\")[-1]\n",
    "\n",
    "model_family=\"decoder\"\n",
    "\n",
    "# max_new_tokens=256         max_new_tokens=max_new_tokens,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b03e29",
   "metadata": {},
   "source": [
    "#### measure maximum batch size a given machine can hadle without OOMing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6504c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt = dataset.input[0]\n",
    "print(f\"Sample prompt length (characters): {len(sample_prompt)}\")\n",
    "model, tokenizer = load_mistral_4bit_model(model_name)\n",
    "optimal_batch_size = find_max_batch_size(model, tokenizer, sample_prompt, max_possible=2048, max_length=1024) # TODO: measure with max_new_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd3f13",
   "metadata": {},
   "source": [
    "first run on 24 Gb GPU: \n",
    "```\n",
    "Sample prompt length (characters): 2437  \n",
    "Trying batch_size = 512... ❌ OOM  \n",
    "Trying batch_size = 256... ❌ OOM  \n",
    "Trying batch_size = 128... ❌ OOM  \n",
    "Trying batch_size = 64... ❌ OOM  \n",
    "Trying batch_size = 32... ✅ success  \n",
    "Trying batch_size = 48... ✅ success  \n",
    "Trying batch_size = 56... ✅ success  \n",
    "Trying batch_size = 60... ❌ OOM  \n",
    "Trying batch_size = 58... ❌ OOM  \n",
    "Trying batch_size = 57... ✅ success  \n",
    "\n",
    "✅ Optimal batch size: 57\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d9e780",
   "metadata": {},
   "source": [
    "second run on 24 Gb GPU: \n",
    "```\n",
    "Sample prompt length (characters): 2437\n",
    "Trying batch_size = 1024... ❌ OOM\n",
    "Trying batch_size = 512... ❌ OOM\n",
    "Trying batch_size = 256... ❌ OOM\n",
    "Trying batch_size = 128... ❌ OOM\n",
    "Trying batch_size = 64... ✅ success\n",
    "Trying batch_size = 96... ✅ success\n",
    "Trying batch_size = 112... ❌ OOM\n",
    "Trying batch_size = 104... ❌ OOM\n",
    "Trying batch_size = 100... ❌ OOM\n",
    "Trying batch_size = 98... ✅ success\n",
    "Trying batch_size = 99... ❌ OOM\n",
    "\n",
    "✅ Optimal batch size: 98\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02089655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory after benchmarking\n",
    "release_all_gpu_memory([\"model\",\"model_pipeline\",\"dataloader\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d986c361",
   "metadata": {},
   "source": [
    "### Run the pipeline on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b81c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_batch_size = 30\n",
    "\n",
    "def build_pipeline(model_name, batch_size, ):\n",
    "    model, tokenizer = load_mistral_4bit_model(model_name)\n",
    "    model_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            batch_size=batch_size,\n",
    "            tokenizer=tokenizer,\n",
    "            #max_length=1024,  \n",
    "            max_new_tokens=512,         # ✅ controls output size\n",
    "            truncation=True,            # ✅ safely truncates long inputs\n",
    "            return_full_text=False,     # ✅ excludes input from output\n",
    "            do_sample=False\n",
    "        )\n",
    "    return model_pipeline\n",
    "\n",
    "model_pipeline = build_pipeline(model_name, optimal_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2847e81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to re-import a function in case you've updated it's code in summarizer.py\n",
    "from importlib import reload\n",
    "import summarizer\n",
    "reload(summarizer)\n",
    "\n",
    "# Now re-import manually\n",
    "from summarizer import batch_generate_interpretations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff5df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n🧠 Running interpretations...\")\n",
    "tstp = datetime.now().strftime(r\"%y.%m.%d-%H\")\n",
    "\n",
    "batch_generate_interpretations(dataset, model_pipeline, save_dir=\"output\",\n",
    "                               input_column=\"input\", output_column=\"interpretation\", \n",
    "                               batch_size=optimal_batch_size)\n",
    "#print(result_df[[\"dream\", \"interpretation\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a0690",
   "metadata": {},
   "source": [
    "# Memory investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0f6439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import globals_snapshot\n",
    "\n",
    "tps = globals_snapshot()\n",
    "\n",
    "q = \"\"\"\n",
    "select type, count(var) as cnt\n",
    "from tps\n",
    "group by type \n",
    "order by cnt desc \n",
    "\n",
    "\"\"\"\n",
    "df = ps.sqldf(q)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
